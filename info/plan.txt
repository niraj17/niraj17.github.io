Domain Expertise : Data Storage, Distributed Systems, Performance !


TODO projects in Rust:
   - Cuckcoo filter (improvement over bloom filter)
   - Recently improved Dijkastra Algorithm
   - RAFT consensus algorithm


Detailed Plan:
      1) DSA - [Naveen Garg's lecture], Then books by Alex Xu (Coding pattern), 150 neetcode questions, 50 Dynamic programming question
      2) Linux kernel and multithreading etc.
      3) Domain (work experience) questions
      4) System Design - 2 books by Alex Xu - Read additional papers/blog as mentioned there.
            - Practice on paper
      5) Behavioural Questions -

==========================================================
- Prepare for domain knowledge:
     - RED Architecture, Intelliflash, Cisco
     - NVMe basics and SPDK 
      - Kubernetes and docker
      - Design patterns - SOLID principle
      - Distributed Systems:
           https://github.com/theanalyst/awesome-distributed-systems?tab=readme-ov-file
           https://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html


- Brendan Gregg's book on performance !
- Do basics of AI using PyTorch with book "Hands-On Machine Learning with Scikit-Learn and PyTorch"

- Educative/AlgoMonster paid plan?


- Keywords: B-epsilon tree, Bloom-filter, Consistent hashing, Merkle Tree

-----+ Work Exp - points to talk ----------
DDN Infinia -  Done from scratch, similar to Amazon dynamo (added distributed lock manager for posix APIs) 
             all userspace,  cluster, etcd, raft, consensus,  Key-value store, b-epsilon tree,         Discovery, CM/IM, SDK
                FUSE/S3/custom sdk         (xAI in PoC mode) 

               Performance debugging: lockstat in distributed lock manager - Created tool to reconcile with multiple node
        
Intelliflash - OpenSolaris, 2 node HA, Old  code, Resource Manager 
                     nVMe namespace reservation 
ACI - raft, sqlite, 3 node cluster 

NetApp - SPM sizer, layout, invention report with KK (Gautam)Activeobject - Darren Sawyer (weta digital), wp1miCompression/dedup (Kesri M, Xin Lee)(Minor) QoS - queuing theory (Naresh Patel, Phil Larson)
IBM - mvfs
Calsoft - Linux, NUMA, false sharing

----- Performance analysis : methods ----
  *** Bottleneck -   Hardware (CPU, Disk, network, IO-Controller, interconnect, Bridges etc) & Software (Algorithms, locks) ***

- Problem statement - 
          - Why do we think there is a problem? Was this ever problem-free? Anything changed recently?
- Diagram  - component of the system
- Workload characteristics
- Resource analysis
- USE method - Utilization, Saturation, Error (for both hardware and software)
- Thread state analysis - runnable, waiting,  swapping, (scheduling issues?)
- On CPU analysis - flame graph and cycles-per-instruction (CPI) ratio => (can indicate memory/cache issue (ex False Sharing?))
- Latency histograms

Tools:
  - Linux tools: top, iostat, vmstat, sar, mpstat, (Hardware: mptutil, pciconf) 
  - dynamic tracing
  - profiling (gprof) - (Terminology: CPU pathlength = CPU utilization/OP rate), 
  - lockstat

--- performance measurements and experiment design ----
- Experiment - Steady state,  Repeatable
- Collecting data - counters
- Comparables - same hardware, version, feature enabled etc
- Load generators - iozone, sfs (NetApp specific - perfstat, perfAnt? )


Theory - Queuing Theory:  Little’s Law: L = λW  (L=average number of requests in a system, λ - average arrival rate, W - average wait time)

------ NetApp knowledge -----------
DAS/SAN/NAS
Protocol for data access - ?? Local (posix), NFS, CIFS,  ....object storage?? s3 protocol?
 
Write Path - data put in NVRAM and memory of ctrl-A, copied to NVRAM of ctrl-B, then ack is sent to client.
           - NVRAM is like a redo log 
           - Once NVRAM limit is reached (half-full? or 10 seconds), data is flushed to disk (CP - consistency point)
           - Every CP creates snapshot
           - WAFL is write anywhere file layout -   (Others - Journaling, Log-structured, etc., B-epsilon tree based design) ("Soft update" in UFS?)
                   - Adv: Better write performance, Versioning, snapshot,  
                   - Dis: Write amplification?
           - ONTAP - msg passing architecture: better for concurrency? simplified code? less lock contention
                    - slight performance overhead
           - Controller crash - peer will take over aggregate and use NVRAM data 



Read Path -  Check block cache/metadata cache (Caching strategy - write-through or write-back cache?)
          - LRU Cache?

Storage efficiency - Zero blocks, compression, dedup

RAID - ??      (Erasure Coding?)
Drive Type - Rotating vs SSD, readahead,
             nVME protocol? (Alleviating disk protocol (scci? SATA?) bottleneck?)  



Storage Tiering  - (Like Flash Cache with special hardware?)
FlexVols and LUN - Thin provisioning 

Recovery Point Objective (RPO) is the maximum amount of data, measured in time, that an organization can 
   afford to lose after a disaster or system failure.
- Synchronous vs Asynchronous mirroring

Other techniques for data reduction - Find similar block and then store differences - https://en.wikipedia.org/wiki/Fuzzy_hashing

Compression - LZOpro (Adjustable Compression Levels, exceptionally fast decompression speed, less memory overhead, In-Place Decompression), 
             
              Zstd (Facebook) achieves better compression than LZO while maintaining a very fast decompression speed, similar to LZO's strength.
                   - https://github.com/facebook/zstd
              Weka (similarity)
